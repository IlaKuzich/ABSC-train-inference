{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Bert_sentiment_analysis.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyP2B0w69AdYFy1Lw0fFC0HD"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Install required dependencies and extensions**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sklearn\n",
    "!pip install PyDrive\n",
    "%load_ext tensorboard\n",
    "!pip install tensorboard"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Import required libs**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "from transformers.models.bert.modeling_bert import BertPooler, BertSelfAttention\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "from google.colab import drive"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Define data utils**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n",
    "    x = (np.ones(maxlen) * value).astype(dtype)\n",
    "    if truncating == 'pre':\n",
    "        trunc = sequence[-maxlen:]\n",
    "    else:\n",
    "        trunc = sequence[:maxlen]\n",
    "    trunc = np.asarray(trunc, dtype=dtype)\n",
    "    if padding == 'post':\n",
    "        x[:len(trunc)] = trunc\n",
    "    else:\n",
    "        x[-len(trunc):] = trunc\n",
    "    return x\n",
    "\n",
    "class Tokenizer4Bert:\n",
    "    def __init__(self, max_seq_len, pretrained_bert_name):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_bert_name)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
    "        sequence = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text))\n",
    "        if len(sequence) == 0:\n",
    "            sequence = [0]\n",
    "        if reverse:\n",
    "            sequence = sequence[::-1]\n",
    "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)\n",
    "\n",
    "\n",
    "class SADataset(Dataset):\n",
    "    def __init__(self, fname, tokenizer):\n",
    "        with open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') as fin:\n",
    "            lines = fin.readlines()\n",
    "\n",
    "        all_data = []\n",
    "        for i in range(0, len(lines), 3):\n",
    "            text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
    "            aspect = lines[i + 1].lower().strip()\n",
    "            polarity = lines[i + 2].strip()\n",
    "\n",
    "            text_indexes = tokenizer.text_to_sequence(text_left + \" \" + aspect + \" \" + text_right)\n",
    "            polarity = int(polarity) + 1\n",
    "\n",
    "            text_len = np.sum(text_indexes != 0)\n",
    "            aspect_len = np.sum(aspect != 0)\n",
    "            concat_bert_indexes = tokenizer.text_to_sequence('[CLS] ' + text_left + \" \" + aspect + \" \" + text_right + ' [SEP] ' + aspect + \" [SEP]\")\n",
    "            concat_segments_indexes = [0] * (text_len + 2) + [1] * (aspect_len + 1)\n",
    "            concat_segments_indexes = pad_and_truncate(concat_segments_indexes, tokenizer.max_seq_len)\n",
    "\n",
    "            text_bert_indexes = tokenizer.text_to_sequence(\"[CLS] \" + text_left + \" \" + aspect + \" \" + text_right + \" [SEP]\")\n",
    "            aspect_bert_indexes = tokenizer.text_to_sequence(\"[CLS] \" + aspect + \" [SEP]\")\n",
    "\n",
    "            data = {\n",
    "                'polarity': polarity,\n",
    "                'concat_bert_indexes': concat_bert_indexes,\n",
    "                'concat_segments_indexes': concat_segments_indexes,\n",
    "                'text_bert_indexes': text_bert_indexes,\n",
    "                'aspect_bert_indexes': aspect_bert_indexes\n",
    "            }\n",
    "\n",
    "            all_data.append(data)\n",
    "        self.data = all_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Load train/test data**\n",
    "\n",
    "ABSC twitter dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drive.mount('/content/gdrive')\n",
    "\n",
    "train_path = F\"/content/gdrive/MyDrive/ML/sentiment/dataset/ABSA/train.raw\"\n",
    "test_path = F\"/content/gdrive/MyDrive/ML/sentiment/dataset/ABSA/test.raw\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Bert models**\n",
    "\n",
    "1. Bert based.\n",
    "  This model uses bert pooler output (b_s, embedding_size) and pass it through dense layer to get prediction.\n",
    "2. Local context foces BERT model.\n",
    "This model create calculate representation from global context and local context and concatenate results to get better result for aspect based sentiment classification. To get more info read original papper. https://www.researchgate.net/publication/335238076_LCF_A_Local_Context_Focus_Mechanism_for_Aspect-Based_Sentiment_Classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BertBased(nn.Module):\n",
    "  def __init__(self, bert, opt):\n",
    "    super(BertBased, self).__init__()\n",
    "    self.bert = bert\n",
    "    self.dropout = nn.Dropout(opt['dropout'])\n",
    "    self.dense = nn.Linear(opt['bert_dim'], opt['polarities_dim'])\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    text_bert_indexes, bert_segment_ids = inputs[0], inputs[1]\n",
    "    bert_output = self.bert(text_bert_indexes, token_type_ids=bert_segment_ids)\n",
    "    pooled_output = self.dropout(bert_output.pooler_output)\n",
    "    out = self.dense(pooled_output)\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, config, opt):\n",
    "    super(SelfAttention, self).__init__()\n",
    "    self.opt = opt\n",
    "    self.config = config\n",
    "    self.SA = BertSelfAttention(config)\n",
    "    self.tanh = torch.nn.Tanh()\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    zero_tensor = torch.tensor(np.zeros((inputs.size(0), 1, 1, self.opt['max_seq_len']),\n",
    "                                            dtype=np.float32), dtype=torch.float32).to(self.opt['device'])\n",
    "    SA_out = self.SA(inputs, zero_tensor)\n",
    "    return self.tanh(SA_out[0])\n",
    "\n",
    "# Local Context Focus\n",
    "class LFC_BERT(nn.Module):\n",
    "  def __init__(self, bert, opt):\n",
    "    super(LFC_BERT, self).__init__()\n",
    "\n",
    "    self.bert_spc = bert\n",
    "    self.opt = opt\n",
    "\n",
    "    self.bert_local = bert\n",
    "    self.dropout = nn.Dropout(opt['dropout'])\n",
    "    self.bert_SA = SelfAttention(bert.config, opt)\n",
    "    self.linear_double = nn.Linear(opt['bert_dim'] * 2, opt['bert_dim'])\n",
    "    self.linear_single = nn.Linear(opt['bert_dim'], opt['bert_dim'])\n",
    "    self.bert_pooler = BertPooler(bert.config)\n",
    "    self.dense = nn.Linear(opt['bert_dim'], opt['polarities_dim'])\n",
    "\n",
    "  def feature_dymanic_mask(self, text_local_indexes, aspect_indexes):\n",
    "    texts = text_local_indexes.cpu().numpy()\n",
    "    asps = aspect_indexes.cpu().numpy()\n",
    "    mask_len = self.opt['SRD']\n",
    "    masked_text_raw_indexes = np.ones((text_local_indexes.size(0), self.opt['max_seq_len'], self.opt['bert_dim']), dtype=np.float32)\n",
    "\n",
    "    for text_i, asp_i in zip(range(len(texts)), range(len(asps))):\n",
    "      asp_len = np.count_nonzero(asps[asp_i]) - 2\n",
    "      try:\n",
    "        asp_begin = np.argwhere(texts[text_i] == asps[asp_i][1])[0][0]\n",
    "      except:\n",
    "        continue\n",
    "      if asp_begin >= mask_len:\n",
    "        mask_begin = asp_begin - mask_len\n",
    "      else:\n",
    "        mask_begin = 0\n",
    "      for i in range(mask_begin):\n",
    "        masked_text_raw_indexes[text_i][i] = np.zeros((self.opt['bert_dim']), dtype=np.float)\n",
    "      for j in range(asp_begin + asp_len + mask_len, self.opt['max_seq_len']):\n",
    "        masked_text_raw_indexes[text_i][j] = np.zeros((self.opt['bert_dim']), dtype=np.float)\n",
    "    masked_text_raw_indexes = torch.from_numpy(masked_text_raw_indexes)\n",
    "    return masked_text_raw_indexes.to(self.opt['device'])\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    text_bert_indexes = inputs[0]\n",
    "    bert_segments_ids = inputs[1]\n",
    "    text_local_indexes = inputs[2]\n",
    "    aspect_indexes = inputs[3]\n",
    "\n",
    "    bert_spc_out = self.bert_spc(text_bert_indexes, token_type_ids=bert_segments_ids)\n",
    "    bert_spc_out = self.dropout(bert_spc_out.last_hidden_state)\n",
    "\n",
    "    bert_local_out = self.bert_local(text_local_indexes)\n",
    "    bert_local_out = self.dropout(bert_local_out.last_hidden_state)\n",
    "\n",
    "\n",
    "    masked_local_text_vec = self.feature_dymanic_mask(text_local_indexes, aspect_indexes)\n",
    "    bert_local_out = torch.mul(bert_local_out, masked_local_text_vec)\n",
    "\n",
    "    out_cat = torch.cat((bert_local_out, bert_spc_out), dim=-1)\n",
    "    mean_pool = self.linear_double(out_cat)\n",
    "    self_attention_out = self.bert_SA(mean_pool)\n",
    "    pooled_out = self.bert_pooler(self_attention_out)\n",
    "    dense_out = self.dense(pooled_out)\n",
    "\n",
    "    return dense_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Create Train Task**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class TrainTask:\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.summary_writer = SummaryWriter(comment=opt['model_name'])\n",
    "\n",
    "        tokenizer = Tokenizer4Bert(opt['max_seq_len'], opt['pretrained_bert_name'])\n",
    "        bert = BertModel.from_pretrained(opt['pretrained_bert_name'])\n",
    "        self.model = opt['model_class'](bert, opt).to(opt['device'])\n",
    "\n",
    "        self.trainset = SADataset(train_path, tokenizer)\n",
    "        self.testset = SADataset(test_path, tokenizer)\n",
    "        self.valset = self.testset\n",
    "\n",
    "    def train(self, criterion, optimizer, train_data_loader, val_data_loader):\n",
    "        max_val_acc = 0\n",
    "        max_val_f1 = 0\n",
    "        global_step = 0\n",
    "        path = None\n",
    "        for i_epoch in range(self.opt['num_epoch']):\n",
    "            logging.info('>' * 100)\n",
    "            logging.info('epoch: {}'.format(i_epoch))\n",
    "            n_correct, n_total, loss_total = 0, 0, 0\n",
    "            # switch model to training mode\n",
    "            self.model.train()\n",
    "            for i_batch, batch in enumerate(train_data_loader):\n",
    "                global_step += 1\n",
    "                # clear gradient accumulators\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                inputs = [batch[col].to(self.opt['device']) for col in self.opt['input_columns']]\n",
    "                outputs = self.model(inputs)\n",
    "                targets = batch['polarity'].to(self.opt['device'])\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "                n_total += len(outputs)\n",
    "                loss_total += loss.item() * len(outputs)\n",
    "                if global_step % self.opt['log_step'] == 0:\n",
    "                    train_acc = n_correct / n_total\n",
    "                    train_loss = loss_total / n_total\n",
    "                    logging.info('loss: {:.4f}, acc: {:.4f}'.format(train_loss, train_acc))\n",
    "\n",
    "            train_acc, train_f1 = self.evaluate_acc_f1(train_data_loader)\n",
    "            val_acc, val_f1 = self.evaluate_acc_f1(val_data_loader)\n",
    "\n",
    "            self.summary_writer.add_scalar(\"Accuracy/train\", train_acc, i_epoch + 1)\n",
    "            self.summary_writer.add_scalar(\"F1/train\", train_f1, i_epoch + 1)\n",
    "\n",
    "            self.summary_writer.add_scalar(\"Accuracy/test\", val_acc, i_epoch + 1)\n",
    "            self.summary_writer.add_scalar(\"F1/test\", val_f1, i_epoch + 1)\n",
    "\n",
    "            logging.info('Accuracy: {}. F1: {}'.format(val_acc, val_f1))\n",
    "\n",
    "            if val_acc > max_val_acc:\n",
    "                max_val_acc = val_acc\n",
    "                max_val_epoch = i_epoch\n",
    "                model_name = self.opt['model_name']\n",
    "                accuracy = round(val_acc, 4)\n",
    "                path = F\"/content/gdrive/MyDrive/ML/sentiment/models/{model_name}_{accuracy}\"\n",
    "                torch.save(self.model.state_dict(), path)\n",
    "                logging.info('>> saved: {}'.format(path))\n",
    "            if val_f1 > max_val_f1:\n",
    "                max_val_f1 = val_f1\n",
    "\n",
    "        return path\n",
    "\n",
    "    def evaluate_acc_f1(self, data_loader):\n",
    "        n_correct, n_total = 0, 0\n",
    "        t_targets_all, t_outputs_all = None, None\n",
    "        # switch model to evaluation mode\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i_batch, t_batch in enumerate(data_loader):\n",
    "                t_inputs = [t_batch[col].to(self.opt['device']) for col in self.opt['input_columns']]\n",
    "                t_targets = t_batch['polarity'].to(self.opt['device'])\n",
    "                t_outputs = self.model(t_inputs)\n",
    "\n",
    "                n_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
    "                n_total += len(t_outputs)\n",
    "\n",
    "                if t_targets_all is None:\n",
    "                    t_targets_all = t_targets\n",
    "                    t_outputs_all = t_outputs\n",
    "                else:\n",
    "                    t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
    "                    t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n",
    "\n",
    "        acc = n_correct / n_total\n",
    "        f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='macro')\n",
    "        return acc, f1\n",
    "\n",
    "    def run(self):\n",
    "        # Loss and Optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = Adam(self.model.parameters(), lr=0.00005)\n",
    "\n",
    "        train_data_loader = DataLoader(dataset=self.trainset, batch_size=self.opt['batch_size'], shuffle=True)\n",
    "        test_data_loader = DataLoader(dataset=self.testset, batch_size=self.opt['batch_size'], shuffle=False)\n",
    "        val_data_loader = DataLoader(dataset=self.valset, batch_size=self.opt['batch_size'], shuffle=False)\n",
    "\n",
    "        best_model_path = self.train(criterion, optimizer, train_data_loader, val_data_loader)\n",
    "        self.model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "        test_acc, test_f1 = self.evaluate_acc_f1(test_data_loader)\n",
    "        logging.info('>> test_acc: {:.4f}, test_f1: {:.4f}'.format(test_acc, test_f1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Start training task**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.device(device)\n",
    "\n",
    "models = {\n",
    "  'bert': BertBased,\n",
    "  'lcf_bert': LFC_BERT\n",
    "}\n",
    "\n",
    "input_columns = {\n",
    "  'bert': ['concat_bert_indexes', 'concat_segments_indexes'],\n",
    "  'lcf_bert': ['concat_bert_indexes', 'concat_segments_indexes', 'text_bert_indexes', 'aspect_bert_indexes']\n",
    "}\n",
    "\n",
    "models_to_evaluate = ['bert', 'lcf_bert']\n",
    "\n",
    "for model in models_to_evaluate:\n",
    "  opt = {\n",
    "    'model_name': model,\n",
    "    'model_class': models[model],\n",
    "    'input_columns': input_columns[model],\n",
    "    'hidden_dim': 300,\n",
    "    'embed_dim': 100,\n",
    "    'num_epoch': 10,\n",
    "    'batch_size': 16,\n",
    "    'log_step': 10,\n",
    "    'polarities_dim': 3,\n",
    "    'max_seq_len': 85,\n",
    "    'pretrained_bert_name': 'bert-base-uncased',\n",
    "    'dropout': 0.1,\n",
    "    'bert_dim': 768,\n",
    "    'device': device,\n",
    "    'SRD': 3\n",
    "  }\n",
    "\n",
    "  train_task = TrainTask(opt)\n",
    "  train_task.run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}